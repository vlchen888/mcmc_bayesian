% o p1, you define y:Z'\to {-1,1}, but this is only for binary data -- you then define both the binary and multiclass models which have different ranges

% o p1, you introduce S, but don't indicate what it's used for. Mention that the label model is give by y_l = S(u_l), l \in Z', with possible corruption by noise

% o p1, you don't need notation e^r for standard basis of \R^k

%o p1, description of Bayesian approach is a bit messy. Something like 'In the Bayesian approach the unknown is viewed as a random variable, whose distribution is determined by combining the labeling model with a probability distribution representing our prior beliefs about the classification. Hierarchical Bayesian methods provide additional flexibility by introducing hyperparameters governing properties of the prior distribution, which are in turn treated as random variables to be inferred.'
. 
% o p1, *larger* weight corresponds to high degree of similarity between nodes

% o p1, 'and the corresponding eigenvectors would be indicators', I would get rid of the word 'the', since any linear combiniation of these is also an eigenvector

% o p1, 'eigenvectors corresponding to small eigenvalues', rather than 'small eigenvectors'

% o p1, 'we assume that the prior distribution of the objective functional...' sentence doesn't really make sense (objective functional usually refers to a functional that the solution would minimize, rather than the solution iteself). Maybe something like 'the eigenvalues and eigenvectors of the graph laplacian are used to construct a probability distribution representing our prior beliefs about the classifying function u in (reference to data model)'

% o p1/2, I'd give some details about the Bayesian approach: 
 - Why is this an appropriate prior distribution? Relate to earlier discussion about the lower eigenvectors
 - Where does this likelihood come from? I'd give the gamma=0 one first and relate to the data model given previously. Then mention that uncertainty in the labels could be represented as additive Gaussian noise, give a new data model y_l=S(u_l)+\eta_l, \eta_l \sim N(0,\gamma^2) iid, and show how this leads to the other likelihood

% o p2, split up models (A,B,C), (D,E) and (F) with a bit of text. mention that the hyperparameters in A,B,C refer to the prior model given in (6), then (D,E) refer to this new prior model, and then (F) can apply to all previous models (A-E)

% o p3, before algorithm 2 define f(u,tau,alpha) with some brief justification (bayes theorem, need to retain hyperparameter-dependent normalization constant, etc)

% o p4, similar as above but for Algorithm 3, noting that normalization constants don't need to be retained

o p3/4, keep notation consistent throughout the algorithms. some notes:
 %- I wouldn't use \xi for the proposal jump variable, since it's already used for N(0,1) variables in the prior, and in the non-centered algorithm
 %- Use hats for the proposal variables rather than introducing a new letter
 %- \nu, \mu aren't good choices to use, as they are typically used for measures in this area
 %- Algorithm 2, first two lines, just initialize u^{(0)} = q_1,. the Fielder vector expressed in the standard basis
 %- Algorithm 2, use \eps_1 and \eps_2, different jump sizes for hyperparameters as in Algorithm 3
 %- Algorithms 1,2, I'd use k=0,...,S for the sample index, as you have done in Algorithm 3
 %- Algorithm, \Phi(T(...)) not \Phi(...) in acceptance probability

% o p5, discussion of truncation, note that the next step in the hierarchical models is to be hierarchical about this truncation level

% o p5, discussion of alpha, 'prior distribution' not 'starting distribution'. Also we aren't expecting alpha to converge to a particular value -- it could just be the case that it isn't strongly informed by the data, and/or is highly correlated with tau. It's hard to tell from only 10k samples, but it does look a little like there has been some learning of alpha in Fig 13 -- there's seems to be a bias towards larger alpha, whereas the prior was uniform.

o Non-centering in general, refer to following paper: https://projecteuclid.org/euclid.ss/1185975637
(Andrew and I should have also have a paper out soon regarding non-centering more focused Bayesian inverse problems, it may be ready for your second interim report)
