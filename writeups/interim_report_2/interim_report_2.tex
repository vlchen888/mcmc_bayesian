\documentclass{siamart1116}
\usepackage{amsmath, amssymb}
%\usepackage{amsmath,amssymb,amsfonts,graphicx,amsthm,dsfont}
%\usepackage{listings}
%\usepackage{courier}
\usepackage{enumerate}
%\usepackage{color}
%\usepackage[usenames,dvipsnames]{xcolor}
%\usepackage{hyperref,tikz,mdframed}
%\hypersetup{colorlinks=true,urlcolor=MidnightBlue,citecolor=PineGreen,linkcolor=BrickRed}

% \lstset{
% 	basicstyle=\small\ttfamily,
% 	keywordstyle=\color{blue},
% 	language=python,
% 	xleftmargin=16pt,
% }
\usepackage{algorithmicx}
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{float}

\textwidth=5.8in
\textheight=9in
\topmargin=-0.5in
\headheight=0in
\headsep=.5in
\hoffset  -.4in
\pagestyle{empty}

\newcommand{\Fp}{\mathbb{F}_p}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\kron}[2]{\left(\frac{#1}{#2}\right)}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\End}{\mathrm{End}}
\newcommand{\SO}{\mathrm{SO}}
\newcommand{\SU}{\mathrm{SU}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\dee}{\mathrm{d}}
\newcommand{\deee}{\textbf{\text{\emph{d}}}}

\newcommand{\md}[1]{\textcolor{cyan}{#1}}

\newcommand{\TheAuthors}{V. Chen\\Advised by A. M. Stuart and  M. M. Dunlop\\This SURF is funded by The Aerospace Corporation}

%\newtheorem{theorem}{Theorem}
%\newtheorem{definition}{Definition}

\graphicspath{ {graphics/} }

\title{SURF 2017 Interim Report 2}
\author{\TheAuthors}
\date{}
\begin{document}
\maketitle
\setlength{\unitlength}{1in}
\setlength{\parindent}{0in}

\section{Introduction}
    Over the past month, I have been implementing different hierarchical algorithms for the semi-supervised clustering problem. In the Bayesian formulation, recall the prior distribution,
    \begin{equation}
    \label{eqn:prior}
    u = \sum_{j=0}^M (\lambda_j + \tau^2)^{-\alpha/2}\xi_j q_j,\quad \xi_j\sim \mathsf{N}(0,1) \quad \text{i.i.d.}
    \end{equation}

    as well as the general form of the prior,

    \begin{equation}
    \label{eqn:general_prior}
    u = \sum_{j=0}^M f(\lambda_j)\xi_j q_j,\quad \xi_j\sim \mathsf{N}(0,1) \quad \text{i.i.d.}
    \end{equation}

    The models to be considered for this project evolve as follows:

    For models (A), (B), and (C), the hyperparameters $\tau,\alpha,M$ refer to the prior in \cref{eqn:prior}

    (A) Use fixed $\tau,\alpha, M=N-1$ to cluster data.\\
    (B) Learn $\tau,\alpha$; fix $M=N-1$.\\
    (C) Learn $\tau,\alpha,M$.

    For models (D) and (E), samples from a new prior are given by $u = \sum_{j=0}^M v_j\xi_j q_j$.

    (D) Learn $\{v_j\}_{j=0}^M$ with $M$ fixed.\\
    (E) Learn $\{v_j\}_{j=0}^M$ and $M$.

    Model (F) can apply to all of the previous models.

    (F) Multiclass, hierarchical on number of classes.

    Model (G) focuses on learning the arbitrary function $f$ defined in \cref{eqn:general_prior}. 

    (G) Learn $f(\lambda), M$.

    So far, I have implemented models (A) through (D). The past month has been devoted to experiments testing individual models and comparing their performances, using metrics such as classification accuracy and variance. Classification accuracy is well defined for the model problems that I tested the data on. These problems are outlined in \cref{sec:model_problems}.

\section{Model problems} \label{sec:model_problems}
    Applications to be considered include the following:
    \subsection{Voting records}
        A data set of the voting records on $16$ votes of $435$ individuals from the U.S. House of Representatives. The data is ordered so that all the representatives from the party are contiguous in the indexing. The graph has $N=435$ nodes, with feature vectors that are $d=16$ dimensional. Each component of the feature vectors is either $+1,-1,$ or $0$ to indicate voting for, voting against, or abstaining on a particular bill, respectively. On this data set, we want to perform binary classification with a subset of labeled data. For this problem, we will use the unnormalized Laplacian with fixed length-scale weights. That is, we will define $w_{ij} = \exp\left({-\frac{d(x_i,x_j)}{2l^2}}\right), \quad d_{p,q}(x,y) = ||x-y||^q_p$ with $p=2,q=2,l=1$. \cref{fig:voting_spec} is part of the spectrum of this choice of Laplacian.

        \begin{figure}[!htb]
        \caption{\label{fig:voting_spec} Lowest $9$ eigenvectors of unnormalized $L$ of voting records.}
        \includegraphics[width=\linewidth]{laplacians/voting_laplacian.png}
        \end{figure}

    \subsection{Two moons}
        This is a synthetic data set constructed with two half circles in $\mathbb{R}^2$ with radius one. These are embedded in $\mathbb{R}^{100}$, and data points are sampled from the circles with Gaussian noise distributed as $\mathsf{N}(0,\sigma^2)$ added to each of the 100 dimensions. In this data set, we will be generating realizations of two moons with $N=1000$ or $N=2000$ nodes, each associated with $d=100$ dimensional feature vectors. This data is again ordered, with the first $N/2$ nodes generated from the first half circle, and the latter $N/2$ from the second. Again, here we want to perform binary classification into the two half circles from which the data was generated. We will be using the self-tuning, symmetric Laplacian for this data set, introduced in \cite{SelfTuning}. This Laplacian matrix has weights that infer the local spatial scale from the data, removing the need to choose a fixed length-scale parameter. The use of this self-tuning Laplacian in the two-moons data set seems to encode more information in the eigenvectors. Compare \cref{fig:moon_spec} with \cref{fig:moon_un_spec}.

        \begin{figure}[!htb]
        \caption{\label{fig:moon_spec} Lowest $9$ eigenvectors of self-tuning symmetric $L$ of two moons, $\sigma = 0.04$.}
        \includegraphics[width=\linewidth]{laplacians/moon_laplacian.png}
        \end{figure}

        \begin{figure}[!htb]
        \caption{\label{fig:moon_un_spec} Lowest $9$ eigenvectors of unnormalized fixed length $L$ of two moons, $\sigma = 0.04$.}
        \includegraphics[width=\linewidth]{laplacians/moon_laplacian_un.png}
        \end{figure}

    \subsection{MNIST data sets}
        This data set contains 70,000 images of $28 \times 28$ pixels with handwritten digits $0$ through $9$. The feature vectors are $d=400$ dimensional and are formed by projection onto the first $50$ PCA components. We will focus on binary classification between the digits $4$ and $9$. We will use the self-tuning, symmetric normalized Laplacian again. See \cref{fig:mnist_spec}.

        \begin{figure}[!htb]
        \caption{\label{fig:mnist_spec} Lowest $9$ eigenvectors of self-tuning symmetric $L$ of MNIST, digits $4$ and $9$.}
        \includegraphics[width=\linewidth]{laplacians/mnist_laplacian.png}
        \end{figure}

\section{Algorithms}
    This section discusses the algorithms that I have implemented for the hierarchical models.

    \subsection{Model (A)}
        For the nonhierarchical model (A), I implemented a pCN algorithm first introduced in \cite{BeRoStVo08} and described in \cref{alg:generalpCN}.

        \begin{algorithm}
        \caption{General pCN adapted from \cite{CoRoStWh13}}
        \label{alg:generalpCN}
        \begin{algorithmic}
        \State{Select $u^{(0)}$. Select $\tau, \alpha$. Select $\beta \in [0, 1]$}
        \For{$k = 0$ to $S$}
        \State{Sample $v$ from the prior distribution given in \cref{eqn:prior}}
        \State{Set $\hat u^{(k)} = (1- \beta^2)^{1/2}u^{(k)} + \beta v$}
        \State{Set $\alpha(u^{(k)} \to \hat u^{(k)}) = \min (1, \exp(\Phi(u^{(k)}) - \Phi(\hat u^{(k)})) )$}
        \State{Set $u^{(k+1)} = \hat u^{(k)}$ with probability $\alpha(u^{(k)} \to \hat u^{(k)})$, and set $u^{(k+1)} = u^{(k)}$ otherwise}
        \EndFor
        \State \Return $\{u^{(k)}\}$
        \end{algorithmic}
        \end{algorithm}

    \subsection{Model (B)}
        To implement model (B), which is hierarchical, we need to sample conditional distributions that govern $u, \tau,$ and $\alpha$. When we attempt to sample the posterior distribution in the hierarchical methods, we could use the Gibbs sampler. The basic form of Gibbs sampling has two repeated steps:
        \begin{itemize}
        \item Update $u^{(n+1)} \sim u|\theta^{(n)}, y$
        \item Update $\theta^{(n+1)} \sim \theta|u^{(n+1)}, y$
        \end{itemize}
        We use $\theta$ to represent the hyperparameters, which could include $\tau, \alpha,$ and $M$. However, we cannot sample the conditional distributions directly, so we replace direct sampling with Markov Chain Monte Carlo (MCMC) indirect sampling methods, which are invariant with respect to each of the exact conditional distributions. With Metropolis-within-Gibbs, at every step we update $u^{(n+1)}, \tau^{(n+1)}$ and $\alpha^{(n+1)}$ each with MCMC to target these conditional distributions. One method is to update $u, \tau, \alpha$ in a block, by proposing $(u,\theta) \to  (\hat u, \hat \theta)$ and computing the transition probability for this step. 

        The algorithms that we will implement will be slightly different, as we will independently propose $u \to \hat u, \tau \to \hat \tau, \alpha \to \hat \alpha, M \to \hat M$, and perform these transitions in separate steps. At each step, we fix all but the proposed parameter, and we compute the transition probability. This is the algorithm that we will be using for the following hierarchical algorithms.

        Our first of two hierarchical algorithms for model (B) is deemed ``centered," as contrasted with the second algorithm described later in this section. Using Metropolis-within-Gibbs, we will require an expression for the posterior, so define $f(u,\tau,\alpha)$ as the joint posterior distribution. By Bayes' theorem, 
        \[f(u,\tau,\alpha) \propto \exp(-\Phi(u))\times\mathbb{P}(u,\tau,\alpha) = \exp(-\Phi(u))\mathbb{P}(u|\theta)\pi_0(\theta).\]
        Recall from \cref{eqn:prior} that the prior is distributed as $\mathsf{N}(0, C)$ with the covariance matrix $C(\theta) = C(\tau, \alpha) = (L + \tau^2I)^{-\alpha}$. We can write

        \begin{equation}
        \label{eqn:centered_post}
        f(u,\tau,\alpha) \propto 
        \frac{1}{\sqrt{(2\pi)^d \det C(\tau,\alpha)}} \exp\left(-\Phi(u)-\frac{1}{2}\langle u, C(\tau,\alpha)^{-1}u  \rangle + \log(\pi_0(\tau,\alpha)) \right).
        \end{equation}
        The normalization constant $\det(C(\theta))$ depends on $\tau, \alpha$ now and does not cancel out. Using this expression for the posterior, I implemented the Metropolis-within-Gibbs method for model (B) in \cref{alg:hierarchical_tau_alpha}.

        \begin{algorithm}
        \caption{Hierarchical on $\tau, \alpha$}
        \label{alg:hierarchical_tau_alpha}
        \begin{algorithmic}
        \State Initialize $u^{(0)} = q_1$, the Fiedler vector expressed in the standard basis.
        \State Initialize $\tau^{(0)}, \alpha^{(0)}$. Select $\beta \in [0, 1]$
        \State Pick $\epsilon_1, \epsilon_2$, the jump sizes for $\tau, \alpha$ respectively.
        \For{$k = 0$ to $S$}
        \State Sample $v$ from the prior distribution and expressed in the eigenbasis \Comment{$u|y, \tau, \alpha$}.
        \State Expressing $u$ in the eigenbasis, set a proposal $\hat u^{(k)} = (1- \beta^2)^{1/2}u^{(k)} + \beta v$
        \State Set $u^{(k+1)} = \hat u^{(k)}$ with probability 
        \[A(u^{(k)} \to \hat u^{(k)}) = \min \left\{1, \exp(\Phi(u^{(k)}) - \Phi(\hat u^{(k)})) \right\}\]

        \State Set a proposal $\hat \tau^{(k)} = \tau^{(k)} + \epsilon_1 t$ for $t \sim \mathsf{N}(0, 1)$ \Comment{$\tau|y,u,\alpha$}
        \State Set $\tau^{(k+1)} = \hat \tau^{(k)}$ with probability 

        \[A(\tau^{(k)} \to \hat \tau^{(k)}) = \min \left\{ 1, \frac{f(u^{(k+1)}, \hat \tau^{(k)}, \alpha^{(k)})}{f(u^{(k+1)}, \tau^{(k)}, \alpha^{(k)})}\right\}\]
        (Using the eigenbasis representation of $u$ for computation) \Comment{$f$ defined in \cref{eqn:centered_post}}

        \State Set a proposal $\hat \alpha^{(k)} = \alpha^{(k)} + \epsilon_2 a$ for $a \sim \mathsf{N}(0, 1)$ \Comment{$\alpha|y,u,\tau$}
        \State Set $\alpha^{(k+1)} = \hat \alpha^{(k)}$ with probability

         \[A(\alpha^{(k)}\to\hat\alpha^{(k)}) = 
         \min\left\{1, 
         \frac{f(u^{(k+1)}, \tau^{(k+1)}, \hat \alpha^{(k)})}
         {f(u^{(k+1)}, \tau^{(k+1)}, \alpha^{(k)})}\right\}
         \]

        \EndFor
        \State \Return $\{u^{(k)}, \tau^{(k)}, \alpha^{(k)}\}$
        \end{algorithmic}
        \end{algorithm}



        We also looked at a different parameterization for model (B). \cref{alg:xi_tau_alpha} uses the variable $\xi$ that is related to the classifying function $u$ by 
        \begin{equation}
        \label{eqn:noncentered_T}
        T(\xi,\tau,\alpha) = \sum_{i=0}^M \frac{1}{(\lambda_i+\tau^2)^{\alpha/2}}\xi_iq_i = u
        \end{equation}

        following \cref{eqn:prior}. This algorithm is ``non-centered" compared to the centered parameterization given in \cref{alg:hierarchical_tau_alpha}, meaning that the unknown classifying variable $\xi$ and the parameters $\theta=(\tau,\alpha)$ are a priori independent \cite{Noncentered}.

        This model assumes the prior $\xi \sim \mathsf{N}(0,I)$. Similar to the centered case, define $g(\xi,\tau,\alpha)$ as the joint posterior distribution. By Bayes' theorem, 
        \[g(\xi,\tau,\alpha) \propto \exp(-\Phi(T(\xi,\tau,\alpha)))\times \mathbb{P}(\xi,\tau,\alpha) = \exp(-\Phi(T(\xi,\tau,\alpha)))\mathbb{P}(\xi)\pi_0(\tau, \alpha)\]
        Note that $\mathbb{P}(\xi) = \frac{1}{\sqrt{(2\pi)^d \det I}} \exp(-\frac{1}{2}\langle \xi, I\xi  \rangle) \propto \exp(-\frac{1}{2}\langle \xi,\xi \rangle)$ and the normalization constants drop out. We obtain:
        \begin{equation}
        \label{eqn:noncentered_post}
        g(\xi,\tau,\alpha) \propto \exp\left( -\Phi(T(\xi,\tau,\alpha))-\frac{1}{2}\langle \xi,\xi \rangle + \log(\pi_0(\tau,\alpha)) \right).
        \end{equation}

        \begin{algorithm}
        \caption{Non-centered parameterization: sampling $\xi, \tau, \alpha$}
        \label{alg:xi_tau_alpha}
        \begin{algorithmic}
        \State Choose $\xi^{(0)} \in \mathbb{R}^N, \alpha^{(0)}, \tau^{(0)} > 0, \beta \in (0, 1]$ and $\epsilon_1, \epsilon_2 > 0$.
        \For{$k=0$ to $S$}
        \State Propose $\hat\xi^{(k)} = (1-\beta^2)^{\frac{1}{2}}\xi^{(k)} + \beta \zeta^{(k)}$, $\zeta^{(k)} \sim \mathsf{N}(0, I)$
        \State Make transition $\xi^{(k)} \to \hat\xi^{(k)}$ with probability
        \[ A(\xi^{(k)} \to \hat\xi^{(k)}) = \min\left\{1, \exp\left(\Phi(T(\xi^{(k)},\tau^{(k)},\alpha^{(k)})) - \Phi(T(\hat\xi^{(k)},\tau^{(k)},\alpha^{(k)}))\right) \right\}\] \Comment{$T$ defined in \cref{eqn:noncentered_T}}

        \State Propose $\hat\tau^{(k)} = \tau^{(k)} + \epsilon_1 \rho^{(k)}, \rho^{(k)} \sim \mathsf{N}(0,1)$
        \State Make transition $\tau^{(k)} \to \hat\tau^{(k)}$ with probability
        \[ A(\tau^{(k)} \to \hat\tau^{(k)}) = \min\left\{1, \frac{g(\xi^{(k+1)},\hat\tau^{(k)},\alpha^{(k)})}{g(\xi^{(k+1)},\tau^{(k)},\alpha^{(k)})} \right\}\] \Comment{$g$ defined in \cref{eqn:noncentered_post}}

        \State Propose $\hat\alpha^{(k)} = \alpha^{(k)} + \epsilon_2 \sigma^{(k)}, \sigma^{(k)} \sim \mathsf{N}(0,1)$
        \State Make transition $\alpha^{(k)} \to \hat\alpha^{(k)}$ with probability
        \[ A(\alpha^{(k)} \to \hat\alpha^{(k)}) = \min\left\{1, \frac{g(\xi^{(k+1)},\tau^{(k+1)},\hat \alpha^{(k)})}{g(\xi^{(k+1)},\tau^{(k+1)},\alpha^{(k)})} \right\}\]
        \EndFor
        \State \Return $\{ T(\xi^{(k)},\tau^{(k)},\alpha^{(k)}), \tau^{(k)}, \alpha^{k} \}$
        \end{algorithmic}
        \end{algorithm}

    \subsection{Model (C)}

\section{Voting records experiments with models (A) and (B)}
    In each of the following experiments, the labeled data $Z'$ was chosen consistently: members 20-30 was labeled one party and members 280-290 as the other.

    For \cref{alg:generalpCN}, fixing $\gamma = 0.0001, \beta = 0.4, \tau = 1, \alpha = 1$, and running for $100000$ iterations with a burn-in period of $1000$, I could get clustering at about 85\% accuracy. One final clustering obtained is shown in \cref{fig:mcmc_gamma_final}, with relevant figures \cref{fig:mcmc_gamma_acceptance} and \cref{fig:mcmc_gamma_senators} that suggest that the chain has converged.
    \begin{figure}[!htb]
    \begin{minipage}{0.48\textwidth}
        \caption{\label{fig:mcmc_gamma_final} \cref{alg:generalpCN} final average}
        \includegraphics[width = \linewidth]{voting/mcmc_gamma/final_avg.png}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \caption{\label{fig:mcmc_gamma_acceptance} \cref{alg:generalpCN} average $u$ acceptance probability}
        \includegraphics[width=\linewidth]{voting/mcmc_gamma/u_accept.png}
    \end{minipage}
    \end{figure}

    \begin{figure}[!htb]
    \caption{\label{fig:mcmc_gamma_senators} Running averages of classifications of delegates}
    \centering
    \includegraphics[width=\linewidth]{voting/mcmc_gamma/senator_avgs.png}
    \end{figure}
    Experiments with \cref{alg:hierarchical_tau_alpha} on the voting records data indicated poor mixing in the hyperparameters by looking at traces of $\tau, \alpha$. We decided that the problem was in part due to the irregularity of the larger eigenvectors, perhaps because of the accuracy of MATLAB's eig solver. Truncating the list of eigenvectors to only consider the first 50 seemed to help the problem, and we were able to observe better clustering. Fixing $\gamma = 0.0001, \beta = 0.4, \tau^{(0)}=20,\alpha^{(0)}=20,\tau\in[0.1,60],\alpha\in[0.1,60],\epsilon_\tau=0.1,\epsilon_\alpha=0.1$ and running $100000$ iterations with a burn-in period of $1000$, the accuracy is about 83\%. See \cref{fig:centered_voting_avg}, \cref{fig:centered_voting_accept}, \cref{fig:centered_voting_tau}, \cref{fig:centered_voting_alpha}.

    \begin{figure}[!htb]
    \begin{minipage}{0.48\textwidth}
        \caption{\label{fig:centered_voting_avg} \cref{alg:hierarchical_tau_alpha} after truncating eigenvectors, final average}
        \includegraphics[width=\linewidth]{voting/centered/final_avg.png}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \caption{\label{fig:centered_voting_accept} \cref{alg:hierarchical_tau_alpha} average $u$ acceptance probability}
        \includegraphics[width=\linewidth]{voting/centered/acceptance_u_probability.png}
    \end{minipage}
    \end{figure}

    \begin{figure}[!htb]
    \begin{minipage}{0.48\textwidth}
        \caption{\label{fig:centered_voting_tau} \cref{alg:hierarchical_tau_alpha} trace of $\tau$}
        \includegraphics[width=\linewidth]{voting/centered/trace_tau.png}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \caption{\label{fig:centered_voting_alpha} \cref{alg:hierarchical_tau_alpha} trace of $\alpha$}
        \includegraphics[width=\linewidth]{voting/centered/trace_alpha.png}
    \end{minipage}

    \end{figure}

    Finally, the non-centered \cref{alg:xi_tau_alpha} seems to converge faster than \cref{alg:hierarchical_tau_alpha}, and truncation of the eigenvectors is not necessary. Fixing $\gamma = 0.0001, \beta = 0.1, \tau^{(0)}=20,\alpha^{(0)}=20,\tau\in[0.1,60],\alpha\in[0.1,60],\epsilon_\tau=1,\epsilon_\alpha=1$ and running $100000$ iterations with a burn-in period of $1000$, the accuracy is about 87\%.  See \cref{fig:noncentered_voting_avg}, \cref{fig:noncentered_voting_accept}, \cref{fig:noncentered_voting_tau}, \cref{fig:noncentered_voting_alpha}.

    \begin{figure}[!htb]
    \begin{minipage}{0.48\textwidth}
        \caption{\label{fig:noncentered_voting_avg}\cref{alg:xi_tau_alpha} final average}
        \includegraphics[width=\linewidth]{voting/noncentered/final_avg.png}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \caption{\label{fig:noncentered_voting_accept} \cref{alg:xi_tau_alpha} average $\xi$ acceptance probablity}
        \includegraphics[width=\linewidth]{voting/noncentered/acceptance_xi_probability.png}
    \end{minipage}
    \end{figure}

    \begin{figure}[!htb]
    \begin{minipage}{0.48\textwidth}
        \caption{\label{fig:noncentered_voting_tau}\cref{alg:xi_tau_alpha} trace of $\tau$}
        \includegraphics[width=\linewidth]{voting/noncentered/trace_tau.png}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \caption{\label{fig:noncentered_voting_alpha} \cref{alg:xi_tau_alpha} trace of $\alpha$}
        \includegraphics[width=\linewidth]{voting/noncentered/trace_alpha.png}
    \end{minipage}
    \end{figure}

\section{Two moons experiments with models (A) and (B)}
    Using these algorithms, I also ran experiments to cluster the two-moons data set, with $r=1, N = 1000, d = 100, \sigma = 0.1$. We used the self-tuning Laplacian introduced in \cite{SelfTuning}.

    For each of the following experiments, we labeled the same $42$ nodes, $21$ in each half-moon, approximately uniformly spaced. Running \cref{alg:generalpCN} on this data set using only the first 50 eigenvectors with $\gamma=0.0001,\beta=0.4,\tau=1,\alpha=1$ with 100000 iterations and a burn-in period of 1000, we could get around 90\% accuracy.
    \begin{figure}[!htb]
        \begin{minipage}{0.48\textwidth}
            \centering
            \caption{\label{fig:moon_mcmc_gamma_avg} \cref{alg:generalpCN} final average}
            \includegraphics[width=\linewidth]{graphics/moons/mcmc_gamma/u_avg.png}
        \end{minipage} \hfill
        \begin{minipage}{0.48\textwidth}
            \centering
            \caption{\label{fig:moon_mcmc_gamma_accept} \cref{alg:generalpCN} average $u$ acceptance probability}
            \includegraphics[width=\linewidth]{graphics/moons/mcmc_gamma/u_accept.png}
        \end{minipage}
    \end{figure}

    We now compare the results from the centered (\cref{alg:hierarchical_tau_alpha}) and non-centered (\cref{alg:xi_tau_alpha}) algorithms, truncating the number of eigenvectors to $50$ in both cases. For both algorithms, we fixed $\gamma = 0.1, \beta = 0.4, \tau^{(0)}=20,\alpha^{(0)}=20,\tau\in[0.01,60],\alpha\in[1,60],\epsilon_\tau=0.5,\epsilon_\alpha=1$ and ran $100000$ iterations with a burn-in period of $1000$. With these settings, \cref{alg:hierarchical_tau_alpha} achieves around 90\% accuracy, while \cref{alg:xi_tau_alpha} achieves around 98\%. It still appears that the non-centered algorithm converges faster than the centered algorithm. If the centered algorithm is initialized at $\tau^{(0)} = 1$, the final classification accuracy is very similar to that of the non-centered algorithm. See \cref{fig:moon_centered_avg}, \cref{fig:moon_centered_accept}, \cref{fig:moon_centered_tau}, \cref{fig:moon_centered_alpha}, \cref{fig:moon_noncentered_avg}, \cref{fig:moon_noncentered_accept}, \cref{fig:moon_noncentered_tau}, \cref{fig:moon_noncentered_alpha}.

    \begin{figure}[!htb]
        \begin{minipage}{0.48\textwidth}
            \centering
            \caption{\label{fig:moon_centered_avg} \cref{alg:hierarchical_tau_alpha} final average}
            \includegraphics[width=\linewidth]{graphics/moons/centered/final_avg.png}
        \end{minipage} \hfill
        \begin{minipage}{0.48\textwidth}
            \centering
            \caption{\label{fig:moon_centered_accept} \cref{alg:hierarchical_tau_alpha} average $u$ acceptance probability}
            \includegraphics[width=\linewidth]{graphics/moons/centered/acceptance_u_probability.png}
        \end{minipage}
    \end{figure}

    \begin{figure}[!htb]
        \begin{minipage}{0.48\textwidth}
            \centering
            \caption{\label{fig:moon_centered_tau} \cref{alg:hierarchical_tau_alpha}, trace $\tau$}
            \includegraphics[width=\linewidth]{graphics/moons/centered/trace_tau.png}
        \end{minipage} \hfill
        \begin{minipage}{0.48\textwidth}
            \centering
            \caption{\label{fig:moon_centered_alpha} \cref{alg:hierarchical_tau_alpha}, trace $\alpha$}
            \includegraphics[width=\linewidth]{graphics/moons/centered/trace_alpha.png}
        \end{minipage}
    \end{figure}

    \begin{figure}[!htb]
        \begin{minipage}{0.48\textwidth}
            \centering
            \caption{\label{fig:moon_noncentered_avg} \cref{alg:xi_tau_alpha} final average}
            \includegraphics[width=\linewidth]{graphics/moons/noncentered/final_avg.png}
        \end{minipage} \hfill
        \begin{minipage}{0.48\textwidth}
            \centering
            \caption{\label{fig:moon_noncentered_accept} \cref{alg:xi_tau_alpha} average $\xi$ acceptance probability}
            \includegraphics[width=\linewidth]{graphics/moons/noncentered/acceptance_xi_probability.png}
        \end{minipage}
    \end{figure}

    \begin{figure}[!htb]
        \begin{minipage}{0.48\textwidth}
            \centering
            \caption{\label{fig:moon_noncentered_tau} \cref{alg:xi_tau_alpha}, trace $\tau$}
            \includegraphics[width=\linewidth]{graphics/moons/noncentered/trace_tau.png}
        \end{minipage} \hfill
        \begin{minipage}{0.48\textwidth}
            \centering
            \caption{\label{fig:moon_noncentered_alpha} \cref{alg:xi_tau_alpha}, trace $\alpha$}
            \includegraphics[width=\linewidth]{graphics/moons/noncentered/trace_alpha.png}
        \end{minipage}
    \end{figure}

    Notice the convergence of $\tau$ and its low variance in the samples for the non-centered self-tuning algorithm.

\section{Experiments comparing hierarchical and nonhierarchical algorithms}

\section{Experiments with model (C)}

\section{Experiments with model (D)}

\bibliographystyle{siamplain}
\bibliography{references}
\end{document}