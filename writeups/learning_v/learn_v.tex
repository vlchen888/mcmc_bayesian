\documentclass{siamart1116}
\usepackage{amsmath, amssymb}
%\usepackage{amsmath,amssymb,amsfonts,graphicx,amsthm,dsfont}
%\usepackage{listings}
%\usepackage{courier}
\usepackage{enumerate}
%\usepackage{color}
%\usepackage[usenames,dvipsnames]{xcolor}
%\usepackage{hyperref,tikz,mdframed}
%\hypersetup{colorlinks=true,urlcolor=MidnightBlue,citecolor=PineGreen,linkcolor=BrickRed}

% \lstset{
%   basicstyle=\small\ttfamily,
%   keywordstyle=\color{blue},
%   language=python,
%   xleftmargin=16pt,
% }
\usepackage{algorithmicx}
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{multicol}

\textwidth=5.8in
\textheight=9in
\topmargin=-0.5in
\headheight=0in
\headsep=.5in
\hoffset  -.4in
\pagestyle{empty}

\newcommand{\Fp}{\mathbb{F}_p}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\kron}[2]{\left(\frac{#1}{#2}\right)}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\End}{\mathrm{End}}
\newcommand{\SO}{\mathrm{SO}}
\newcommand{\SU}{\mathrm{SU}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\dee}{\mathrm{d}}
\newcommand{\deee}{\textbf{\text{\emph{d}}}}

\newcommand{\md}[1]{\textcolor{cyan}{#1}}

\newcommand{\TheAuthors}{V. Chen}

%\newtheorem{theorem}{Theorem}
%\newtheorem{definition}{Definition}

\graphicspath{ {graphics/} }

\title{Model D: Learning $v_j$}
\author{\TheAuthors}
\date{}
\begin{document}
\maketitle
\setlength{\unitlength}{1in}
\setlength{\parindent}{0in}
\section{Uniform prior on $v$}
    This algorithm reparameterizes the problem in terms of the random vectors $v$ and $\xi$. $v_j$ modifies the scale of influence of $q_j$, the $j$th eigenvector of the graph Laplacian, on the classifying function $u$.
    \begin{equation}
    \label{eqn:v_T}
    T(v,\xi) = \sum_{i=0}^{M} v_i\xi_iq_i = u
    \end{equation}
    Here, $M$ is fixed. We take $\xi \sim \mathsf{N}(0, I)$. Using good estimates for $\tau, \alpha$, perhaps obtained by algorithms that learn $\tau, \alpha$, set the prior on $v$ to be:
    \[v_j \sim \mathsf{U}\left((1-a)(\lambda_j+\tau^2)^{-\alpha/2},(1+a)(\lambda_j+\tau^2)^{-\alpha/2}\right)\]
    where $a$ is a fixed scalar.

    Finally, we derive an expression for the posterior with Bayes' theorem.
    \begin{align*}
    \mathbb{P}(v,\xi | y) &\propto \mathbb{P}(y|v, \xi) \mathbb{P}(v, \xi)\\
    &\propto \exp \left(-\Phi(T(v,\xi)) \right) \mathbb{P}(v)\mathbb{P}(\xi) \\
    &\propto \exp \left(-\Phi(T(v,\xi)) + \log (\pi_0(v)) - \frac{1}{2}\langle \xi, \xi \rangle  \right)
    \end{align*}

    Let $h(v,\xi)$ denote the joint posterior on $v$ and $\xi$. Then,
    \begin{equation}
    \label{eqn:learn_v_posterior}
    h(v, \xi) \propto \exp \left(-\Phi(T(v,\xi)) + \log (\pi_0(v)) - \frac{1}{2}\langle \xi, \xi \rangle  \right).
    \end{equation}

    \begin{algorithm}
    \caption{Non-centered parameterization, hierarchical with $v$}
    \label{alg:hier_v}
    \begin{algorithmic}
    \State Choose $v^{(0)}, \xi^{(0)} \in \mathbb{R}^N, \beta \in (0, 1], \epsilon > 0$.
    \For{$k=0$ to $S$}
    \State Propose $\hat\xi^{(k)} = (1-\beta^2)^{\frac{1}{2}}\xi^{(k)} + \beta \zeta^{(k)}$, $\zeta^{(k)} \sim \mathsf{N}(0, I)$
    \State Make transition $\xi^{(k)} \to \hat\xi^{(k)}$ with probability
    \[ A(\xi^{(k)} \to \hat\xi^{(k)}) = \min\left\{1, \exp\left(\Phi(T(v^{(k)}, \xi^{(k)})) - \Phi(T(v^{(k)}, \hat \xi^{(k)}))\right) \right\}\]

    \State Propose $\hat v^{(k)} = v^{(k)} + \epsilon \rho^{(k)}, \rho^{(k)} \sim \mathsf{N}(0,I)$
    \If {$\hat v^{(k)} _j$ is outside of its prior interval for any $j$} 
        \State Reject and set $v^{(k+1)} = v^{(k)}$.
    \Else
    \State Make transition $v^{(k)} \to \hat v^{(k)}$ with probability
    \begin{align*}
     A(v^{(k)} \to \hat v^{(k)}) &= \min\left\{1, \frac{h(\hat v^{(k)}, \xi^{(k+1)})}{h(v^{(k)}, \xi^{(k+1)})}\right\} \\
     &= \min\left\{1, \exp\left(\Phi(T(v^{(k)}, \xi^{(k+1)}))-\Phi(T(\hat v^{(k)}, \xi^{(k+1)})) \right) \right\}
     \end{align*}
    \EndIf

    \EndFor
    \State \Return $\{ T(v^{(k)},\xi^{(k)}), v^{(k)}, \xi^{(k)} \}$
    \end{algorithmic}
    \end{algorithm}

\section{Gaussian prior on $v$}
    We can also take a Gaussian prior on $v_j$. With fixed $\tau_0, \alpha_0$, take the prior 
    \[v_j \sim \mathsf{N}\left(\frac{1}{(\lambda_j + \tau_0^2)^{\alpha_0/2}},\sigma_j^2\right)\]
    so that in all, $v \sim \mathsf{N}{(\mu, D)}$ with $\mu_j = \frac{1}{(\lambda_j + \tau_0^2)^{\alpha_0/2}}$ and $D = \text{diag}(\sigma_j^2)$. Again, we derive an expression for the joint posterior $h(v,\xi)$:
    \begin{align*}
        \mathbb{P}(v,\xi|y) &\propto \mathbb{P}(y|v,\xi) \mathbb{P}(v,\xi)\\
        &\propto \exp \left(-\Phi(T(v,\xi)) \right) \mathbb{P}(v) \mathbb{P}(\xi)\\
        &\propto \exp \left(-\Phi(T(v,\xi)) -\frac{1}{2}\langle v - \mu, D^{-1}(v - \mu) \rangle -\frac{1}{2} \langle \xi, \xi \rangle\right).
    \end{align*}
    And we obtain:
    \begin{equation}
    h(v,\xi) = \exp \left(-\Phi(T(v,\xi)) -\frac{1}{2}\langle v - \mu, D^{-1}(v - \mu) \rangle -\frac{1}{2} \langle \xi, \xi \rangle\right).
    \end{equation}

    \begin{algorithm}
    \caption{Non-centered parameterization, hierarchical with $v$, Gaussian prior on $v$}
    \label{alg:hier_v_gaussian}
    \begin{algorithmic}
    \State Choose $v^{(0)}, \xi^{(0)} \in \mathbb{R}^N, \beta_1, \beta_2 \in (0, 1]$.
    \For{$k=0$ to $S$}
    \State Propose $\hat\xi^{(k)} = (1-\beta_1^2)^{\frac{1}{2}}\xi^{(k)} + \beta_1 \zeta^{(k)}$, $\zeta^{(k)} \sim \mathsf{N}(0, I)$
    \State Make transition $\xi^{(k)} \to \hat\xi^{(k)}$ with probability
    \[ A(\xi^{(k)} \to \hat\xi^{(k)}) = \min\left\{1, \exp\left(\Phi(T(v^{(k)}, \xi^{(k)})) - \Phi(T(v^{(k)}, \hat \xi^{(k)}))\right) \right\}\]

    \State Propose $\hat v^{(k)} = \mu + (1-\beta_2^2)^{\frac{1}{2}}(v-\mu) + \beta_2 \rho^{(k)}, \rho^{(k)} \sim \mathsf{N}(0,D)$

    \State Make transition $v^{(k)} \to \hat v^{(k)}$ with probability
    
    \[ A(v^{(k)} \to \hat v^{(k)}) = \min\left\{1, \exp\left(\Phi(T(v^{(k)}, \xi^{(k+1)}))-\Phi(T(\hat v^{(k)}, \xi^{(k+1)})) \right) \right\} \]
     

    \EndFor
    \State \Return $\{ T(v^{(k)},\xi^{(k)}), v^{(k)}, \xi^{(k)} \}$
    \end{algorithmic}
    \end{algorithm}

\section{Learning $M$}
    We can try to learn $M$. Define:
    \begin{equation}
    \label{eqn:v_M_T}
    T(v,\xi,M) = \sum_{i=0}^{M} v_i\xi_iq_i = u.
    \end{equation}
    Taking the uniform prior on $v$, we obtain:
    \begin{equation}
    \label{eqn:learn_v_M_posterior}
    h(v, \xi, M) \propto \exp \left(-\Phi(T(v,\xi, M)) + \log (\pi_0(v, M)) - \frac{1}{2}\langle \xi, \xi \rangle  \right).
    \end{equation}

    \begin{algorithm}
    \caption{Non-centered parameterization, hierarchical with $v, M$}
    \label{alg:hier_v_M}
    \begin{algorithmic}
    \State Choose $v^{(0)}, \xi^{(0)} \in \mathbb{R}^N, M^{(0)}, \beta \in (0, 1], \epsilon > 0$.
    \For{$k=0$ to $S$}
    \State Propose $\hat\xi^{(k)} = (1-\beta^2)^{\frac{1}{2}}\xi^{(k)} + \beta \zeta^{(k)}$, $\zeta^{(k)} \sim \mathsf{N}(0, I)$
    \State Make transition $\xi^{(k)} \to \hat\xi^{(k)}$ with probability
    \[ A(\xi^{(k)} \to \hat\xi^{(k)}) = \min\left\{1, \exp\left(\Phi(T(v^{(k)}, \xi^{(k)}, M^{(k)})) - \Phi(T(v^{(k)}, \hat \xi^{(k)}, M^{(k)}))\right) \right\}\]

    \State Propose $\hat v^{(k)} = v^{(k)} + \epsilon \rho^{(k)}, \rho^{(k)} \sim \mathsf{N}(0,I)$
    \If {$\hat v^{(k)} _j$ is outside of its prior interval for any $j$} 
        \State Reject and set $v^{(k+1)} = v^{(k)}$.
    \Else
    \State Make transition $v^{(k)} \to \hat v^{(k)}$ with probability
    \begin{align*}
     A(v^{(k)} \to \hat v^{(k)}) &= \min\left\{1, \frac{h(\hat v^{(k)}, \xi^{(k+1)}, M^{(k)})}{h(v^{(k)}, \xi^{(k+1)}, M^{(k)})}\right\} \\
     &= \min\left\{1, \exp\left(\Phi(T(v^{(k)}, \xi^{(k+1)},M^{(k)}))-\Phi(T(\hat v^{(k)}, \xi^{(k+1)}, M^{(k)})) \right) \right\}
     \end{align*}
    \EndIf

    \State Propose $\hat M^{(k)} = M^{(k)} + Q$, with jump $Q$ distributed as $\mathbb{P}(Q=k) \propto \frac{1}{1+|k|}$, $|Q|$ bounded.
    \State Make transition $M^{(k)} \to \hat M^{(k)}$ with probability

    \begin{align*}
    A(M^{(k)} \to \hat M^{(k)}) &= \min\left\{1, \frac{h( v^{(k+1)}, \xi^{(k+1)}, \hat M^{(k)}  )}{h( v^{(k+1)}, \xi^{(k+1)}, M^{(k)} )} \right\}\\
    &=\min\left\{1, \exp\left(\Phi(v^{(k+1)}, \xi^{(k+1)}, M^{(k)} ) - \Phi(v^{(k+1)}, \xi^{(k+1)}, \hat M^{(k)} )\right)\right\}
    \end{align*}

    \EndFor
    \State \Return $\{ T(v^{(k)},\xi^{(k)}), v^{(k)}, \xi^{(k)} \}$
    \end{algorithmic}
    \end{algorithm}

\section{Gamma prior}
    Take $l_j = \frac{1}{v_j^2}$, and assume that $l_j \sim \Gamma(\alpha, \beta)$. This noncentered parameterization relates $v, \xi$ to $u$ by

    \begin{equation}
    \label{eqn:gamma_T}
    T(v, \xi) = \sum_{i=0}^M \frac{1}{\sqrt{l_j}}\xi_jq_j = u
    \end{equation}
    We can derive the posterior once again:
    \begin{align*}
    \mathbb{P}(l,\xi | y) &\propto \mathbb{P}(y|l,\xi)\mathbb{P}(\xi)\mathbb{P}(l)\\
    &\propto \exp\left(-\Phi(T(l,\xi)) -\frac{1}{2}\langle\xi, \xi \rangle\right)x^{\alpha - 1}\exp(-\beta x)
    \end{align*}
\bibliographystyle{siamplain}
\bibliography{references}
\end{document}